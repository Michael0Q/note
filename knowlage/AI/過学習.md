# 過学習 (Overfitting)

## 1. 概要

過学習とは、機械学習モデルが訓練データに対して過剰に最適化され、訓練データでは高い精度を示すものの、未知の新しいデータ（テストデータや実世界のデータ）に対しては精度が大幅に低下する現象を指す。モデルがデータの本質的なパターンではなく、訓練データに固有のノイズや偶然の変動まで学習してしまった結果生じる。

## 2. なぜ問題か

過学習したモデルは**汎化性能（Generalization）**が低い。機械学習の目的は、未知のデータに対して正確な予測や分類を行うことであり、過学習したモデルはその目的を達成できない。例えば、スパムメールフィルタが訓練データに含まれる特定の単語の組み合わせに過剰に反応するようになり、本来スパムではない重要なメールまでスパムと誤判定してしまう、といった問題が発生する。

## 3. 原因

過学習は主に以下の要因によって引き起こされる。

- **複雑すぎるモデル**:
  モデルの表現力が高すぎる（例：ニューラルネットワークの層やノードが多すぎる、決定木の深度が深すぎる）と、訓練データの細部まで捉えようとし、ノイズまで学習してしまう。

- **データ量の不足**:
  訓練データの量が少ないと、データが持つ本来の分布を正確に表現できず、モデルは手元の限られたデータの特徴に強く依存してしまう。

- **訓練期間が長すぎる**:
  ニューラルネットワークなどの反復的な学習において、訓練を長く続けるほどモデルは訓練データに適合していくが、ある時点を超えると汎化性能が低下し始め、過学習に陥る。

## 4. 対策

過学習を抑制し、モデルの汎化性能を高めるためには、以下のような手法が用いられる。

- **訓練データを増やす**:
  最も根本的で効果的な対策。多様なデータを学習させることで、モデルはより本質的なパターンを捉えやすくなる。データオーグメンテーション（Data Augmentation）も有効である。

- **モデルの単純化**:
  モデルの複雑さを抑える。

  - ニューラルネットワーク：層やノードの数を減らす。
  - 決定木：木の深さを制限する（Pruning）。

- **正則化 (Regularization)**:
  モデルの複雑さにペナルティを課すことで、過学習を防ぐ手法。

  - **L1 正則化 (Lasso)**: パラメータの一部を完全に 0 にすることができ、特徴選択の効果も持つ。
  - **L2 正則化 (Ridge)**: パラメータが大きな値をとることを防ぎ、滑らかなモデルを促す。

- **ドロップアウト (Dropout)**:
  ニューラルネットワークで用いられる手法。訓練中にランダムに一部のニューロンを無効化することで、各ニューロンが特定のニューロンに依存しすぎることを防ぎ、より頑健な特徴を学習するよう促す。

- **早期終了 (Early Stopping)**:
  訓練データに対する誤差（損失）は減少し続ける一方で、検証データ（Validation Set）に対する誤差が増加に転じた時点で訓練を停止する手法。

---

**参考文献:**

- Song, D., Zhang, Y., Shan, X., Cui, J., & Wu, H. (2017). “Over-Learning” Phenomenon of Wavelet Neural Networks in Remote Sensing Image Classifications with Different Entropy Error Functions. _Entropy_, _19_(3), 101. [https://doi.org/10.3390/e19030101](https://doi.org/10.3390/e19030101)
- Shibata, K., Sasaki, Y., Bang, J. W., Walsh, E. G., Machizawa, M. G., Tamaki, M., ... & Watanabe, T. (2017). Overlearning hyperstabilizes a skill by rapidly making neurochemical processing inhibitory-dominant. _Nature neuroscience_, _20_(3), 470-475.
