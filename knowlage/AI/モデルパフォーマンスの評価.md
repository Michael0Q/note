# パフォーマンスの評価

Document AI のカスタムモデルをトレーニングした後、その性能が実用に足るものかを確認するために不可欠なのが「パフォーマンス評価」です。評価を通じて、モデルがどれだけ正確に情報を抽出できるかを客観的な指標で把握し、改善点を見つけ出します。

---

### 主要な評価指標

モデルの評価には、主に以下の 3 つの指標が用いられます。これらは、テストデータセット（モデルが学習中に見ていないデータ）を使って計算されます。

- **適合率 (Precision)**
- **再現率 (Recall)**
- **F1 スコア (F1 Score)**

これらの指標を理解するために、まず以下の用語を定義します。

- **True Positive (TP)**: モデルが正しく抽出したフィールド。（例：請求書番号を正しく抽出）
- **False Positive (FP)**: モデルが誤って抽出したフィールド。（例：電話番号を請求書番号として抽出）
- **False Negative (FN)**: モデルが抽出できなかった、本来抽出されるべきフィールド。（例：請求書番号を見逃してしまう）

[[モデルの評価で何が起こっているか]]

#### 適合率 (Precision)

$$
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
$$

適合率は、**モデルが抽出したもののうち、どれだけが正しかったか** を示す指標です。「偽の陽性」をどれだけ抑えられたかを表し、この値が高いほど、モデルの抽出結果の信頼性が高いと言えます。

- **適合率が高い**: 抽出されたデータはほとんど正しいが、一部見逃しがあるかもしれない。
- **適合率が低い**: 不要な情報まで抽出している可能性が高い。

#### 再現率 (Recall)

$$
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
$$

再現率は、**抽出されるべきもののうち、どれだけをモデルが抽出できたか** を示す指標です。「偽の陰性」をどれだけ抑えられたかを表し、この値が高いほど、モデルの網羅性が高いと言えます。

- **再現率が高い**: 抽出漏れは少ないが、誤った情報も一緒に抽出しているかもしれない。
- **再現率が低い**: 多くの情報を見逃している可能性が高い。

> **トレードオフの関係**
> 一般的に、適合率と再現率はトレードオフの関係にあります。一方を上げようとすると、もう一方が下がることがよくあります。例えば、抽出の基準を緩めると再現率は上がりますが、誤ったものまで拾ってしまい適合率は下がります。

#### F1 スコア (F1 Score)

$$
\text{F1 Score} = 2 \times \frac{\text{適合率} \times \text{再現率}}{\text{適合率} + \text{再現率}}
$$

F1 スコアは、**適合率と再現率の調和平均**です。両方のバランスを取った総合的な評価指標であり、モデルの全体的な精度を評価する際によく参照されます。0 から 1 の値をとり、1 に近いほど性能が良いとされます。

---

### 評価の手順

1.  **[Evaluate & test] タブに移動**:
    トレーニングが完了したプロセッサの管理画面で、このタブを選択します。
2.  **バージョンの選択**:
    評価したいモデルのバージョンを選択します。
3.  **評価指標の確認**:
    画面には、F1 スコア、適合率、再現率が、ドキュメント全体および各ラベル（フィールド）ごとに表示されます。特定のラベルのスコアが低い場合、そのラベルの学習データに問題（例：データ不足、ラベリングの不一致）がある可能性が考えられます。
4.  **テストドキュメントでの検証**:
    **[Upload Test Document]** から、トレーニングやテストに使用していない未知のドキュメントをアップロードして、実際の抽出結果を視覚的に確認します。これにより、指標だけでは分からないモデルの振る舞い（どのような間違いをするかなど）を把握できます。

---

[[DocumentAIのファインチューニングプロセス]]
