# 概要

ファインチューニング（Fine-tuning）とは、既に大規模なデータセットで[[学習]]済みの[[モデル]]（事前学習済みモデル）を、特定のタスクやドメインに適応させるために、より小規模でタスクに特化したデータセットを使って追加で[[学習]]させる手法です。これにより、汎用的な知識を持つモデルを、専門的なタスクで高いパフォーマンスを発揮するようにカスタマイズすることができます。

## 目的と利点

ファインチューニングには、ゼロから[[モデル]]を[[学習]]させる場合と比較して、以下のような大きな利点があります。

- **効率性**: 大規模なモデルをゼロから[[学習]]するには、膨大なデータと計算リソース、そして長い時間が必要です。ファインチューニングでは、既存のモデルの知識を活用するため、開発時間とコストを大幅に削減できます。
- **性能向上**: 特定のドメイン（例：医療、法律、金融）やタスク（例：特定の文体での文章生成、専門用語の翻訳）において、汎用モデルよりも高い精度と信頼性を実現できます。
- **カスタマイズ性**: モデルの出力のトーン、スタイル、フォーマットなどを、特定の要件に合わせて調整することが可能です。

## プロセス

一般的なファインチューニングのプロセスは、以下のステップで構成されます。

1.  **事前学習済み[[モデル]]の選択**: ベースとなるモデル（例：GPT、BERT、ResNet など）を、目的のタスクに合わせて選択します。
2.  **データセットの準備**: タスクに特化した、質の高いデータセットを収集・作成します。
3.  **データの前処理**: 収集したデータをクレンジングし、モデルが[[学習]]しやすい形式に整えます（正規化、トークン化など）。
4.  **モデルの再[[学習]]**: 準備したデータセットを使い、事前学習済みモデルの重みを更新します。この際、[[学習]]率などのハイパーパラメータの調整が重要になります。
5.  **評価**: テスト用のデータセットを用いて、ファインチューニングされたモデルの性能を評価します。
6.  **デプロイ**: 性能要件を満たしたモデルを、実際のアプリケーションに組み込みます。

## 主な手法

ファインチューニングにはいくつかの手法があり、タスクやリソースに応じて使い分けられます。

- **Full Fine-Tuning**: [[モデル]]のすべての層の重みを更新する手法。モデル全体を新しいタスクに最適化できますが、計算コストが高く、破滅的忘却（後述）のリスクがあります。
- **Parameter-Efficient Fine-Tuning (PEFT)**: [[モデル]]の一部のパラメータのみを更新することで、計算効率を高める手法の総称です。
  - **Partial Fine-Tuning**: モデルの最後の数層のみを[[学習]]させるなど、一部の層に限定して重みを更新します。
  - **Adapters**: 事前学習済みモデルの層の間に、小さなニューラルネットワーク（アダプター）を挿入し、このアダプター部分のみを[[学習]]させます。
  - **LoRA (Low-Rank Adaptation)**: モデルの重みの「更新部分」を低ランク行列で近似することで、[[学習]]対象のパラメータ数を劇的に削減する手法。近年、大規模言語モデルのファインチューニングで広く利用されています。
- **Reinforcement Learning from Human Feedback (RLHF)**: 人間からのフィードバック（評価）を強化学習の報酬として利用し、モデルの出力を人間の意図や好みに沿うように調整する手法です。

[[モデルの重みって？⭐️]]

## 主なユースケース

ファインチューニングは、特定のプロンプトに対して、より望ましい出力を生成するよう[[モデル]]を特化させたい場合に特に有効です。

- **顧客サポート**: 企業の製品情報や過去の問い合わせ履歴を[[学習]]させ、より的確な回答を生成するチャットボットを構築する。
- **コンテンツ生成**: 特定のブランドの文体やトーンを[[学習]]させ、マーケティングコピーや SNS 投稿を自動生成する。
- **専門分野の翻訳**: 法律や医療などの専門用語を含むコーパスで[[学習]]させ、高精度な翻訳[[モデル]]を構築する。
- **感情分析**: 特定の業界のレビューデータを[[学習]]させ、製品やサービスに対する顧客の感情をより正確に分類する。

## 課題と注意点

- **過[[学習]] (Overfitting)**: 小規模なデータセットで[[学習]]させすぎると、そのデータに過剰に適合してしまい、未知のデータに対する汎化性能が低下することがあります。
- **破滅的忘却 (Catastrophic Forgetting)**: 新しいタスクを[[学習]]する過程で、事前学習で得た汎用的な知識を忘れてしまう現象。[[学習]]率を適切に設定するなどの対策が必要です。
- **データセットの品質**: ファインチューニングの成否は、使用するデータセットの質と量に大きく依存します。「Garbage in, garbage out（ゴミを入れればゴミしか出てこない）」の原則が当てはまります。
- **バイアスの継承と増幅**: 事前学習済み[[モデル]]が持つバイアスは、ファインチューニング後も引き継がれ、場合によっては増幅される可能性があります。

## 参考文献

- https://dev.to/shriyansh_iot_98734929139/what-is-fine-tuning-in-ai-1cm9
- https://www.usedojo.ai/blog/fine-tuning-guide
- https://www.coursera.org/articles/what-is-fine-tuning
- https://medium.com/@prabhuss73/fine-tuning-ai-models-a-guide-c515bcd4b580
- https://michielh.medium.com/fine-tuning-ai-for-dummies-a-simplified-guide-to-mastering-model-optimization-ae075ee06649
- https://medium.com/@ardiansyahnasir56/fine-tuning-optimizing-ai-models-for-specific-tasks-04764524adb1
